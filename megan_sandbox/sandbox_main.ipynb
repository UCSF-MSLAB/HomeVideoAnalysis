{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad79b49-59de-49f7-bcc9-5fd9ba0a7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07686078-18da-4217-b40d-02206ee3da7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import functions from sandbox src code files \n",
    "from sandbox_main_src_funs.merge_mp_yolo_dfs import (merge_mp_pose_world, clean_mp_yolo_missing_data, add_orientation_and_turn_direction, save_merge_mp_yolo_df)\n",
    "\n",
    "from sandbox_main_src_funs.frames_to_time import (get_frames_per_second, add_time_column, save_df_w_time)\n",
    "\n",
    "from sandbox_main_src_funs.landmark_visibility import (mp_vis_all_labels_boxplot, mp_vis_lineplot, mp_save_vis_stats_by_label, yolo_vis_lineplot)\n",
    "\n",
    "from sandbox_main_src_funs.segment_video_walk_turn import filter_landmark_single_axis, segment_video_walks_turn\n",
    "\n",
    "from sandbox_main_src_funs.filter_interpolate_funs import (interpolate_landmark_single_axis, \n",
    "filter_landmark_single_axis) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2851e5-2584-4ebb-b022-3120cc0e2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input - eventually will need to edit this to incorporate into main hva pipeline \n",
    "\n",
    "# path to video \n",
    "vid_in_path = r'C:\\Users\\mmccu\\Box\\MM_Personal\\5_Projects\\BoveLab\\3_Data_and_Code\\hva_code\\HomeVideoAnalysis\\tests\\fixtures\\all_videos\\RB_HC_practice videos\\RB_HC_gait_vertical_right.MOV' # vid_in_path set during process_dir() of run.py\n",
    "\n",
    "# load mp, mp_world, and yolo .csv files for one video (from main branch, output of home video analysis run.py 8/29/2024)\n",
    "mp_pose_filepath = r'..\\temp\\main_branch_outputs\\000_run\\RB_HC_gait_vertical_right_mediapipe.csv'\n",
    "mp_world_filepath = r'..\\temp\\main_branch_outputs\\000_run\\RB_HC_gait_vertical_right_mediapipe_world.csv'\n",
    "yolo_filepath = r'..\\temp\\main_branch_outputs\\000_run\\RB_HC_gait_vertical_right_yolo.csv'\n",
    "\n",
    "mp_pose_basename = os.path.splitext(os.path.basename(mp_pose_filepath))[0]\n",
    "mp_all_filepath = mp_pose_basename + '_all.csv'\n",
    "\n",
    "# output folder \n",
    "output_parent_folder = r'..\\temp\\test_sandbox_pipeline_outputs'\n",
    "\n",
    "# read csv \n",
    "mp_pose_df = pd.read_csv(mp_pose_filepath)\n",
    "mp_world_df = pd.read_csv(mp_world_filepath)\n",
    "yolo_df = pd.read_csv(yolo_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07be017-b74b-4180-996c-fd2d51885a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input for new run.py raw pose data outpr\n",
    "raw_pose_data_path = r'C:\\Users\\mmccu\\Box\\MM_Personal\\5_Projects\\BoveLab\\3_Data_and_Code\\data_bw_participants\\test_folder_structure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e62e399-ab8e-4ca9-aeb2-5765676dcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 001 - merge mp df, add tasks info and negative Y\n",
    "[mp_all_df, yolo_df] = merge_mp_pose_world(mp_pose_df, mp_world_df, yolo_df)\n",
    "[mp_all_df, yolo_df] = clean_mp_yolo_missing_data(mp_all_df, yolo_df)\n",
    "[mp_all_df, yolo_df] = add_orientation_and_turn_direction(vid_in_path, mp_all_df, yolo_df)\n",
    "save_merge_mp_yolo_df(mp_all_df, yolo_df, vid_in_path, output_parent_folder)\n",
    "\n",
    "# outputs \n",
    "# mp_all_df and yolo_df pandas data frames updated and .csv file saved    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd9bd02-f6d1-49fa-bcd5-4b36f4dcf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 002 - get frames per second and add time column \n",
    "fps = get_frames_per_second(vid_in_path)\n",
    "[mp_all_df, yolo_df] = add_time_column(mp_all_df, yolo_df, fps)\n",
    "save_df_w_time(mp_all_df, yolo_df, vid_in_path, output_parent_folder)\n",
    "# outputs \n",
    "# fps = video frames per second \n",
    "# [mp_all_df, yolo_df]: panda data frames added seconds and .csv file saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24910c35-3bbb-48df-a755-c88581642f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is na: skipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmccu\\Box\\MM_Personal\\5_Projects\\BoveLab\\3_Data_and_Code\\hva_code\\HomeVideoAnalysis\\megan_sandbox\\sandbox_main_src_funs\\landmark_visibility.py:150: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  vis_stats_df = pd.concat([vis_stats_df, current_vis_stats_row])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip: no_labels_tracked, included in each label\n"
     ]
    }
   ],
   "source": [
    "# step 003 - plot and save landmark visibility scores \n",
    "\n",
    "# mediapipe \n",
    "# boxplot \n",
    "mp_boxplot = mp_vis_all_labels_boxplot(mp_all_df, vid_in_path, output_parent_folder)\n",
    "\n",
    "# lineplot \n",
    "mp_lineplot = mp_vis_lineplot(mp_all_df, vid_in_path, output_parent_folder)\n",
    "\n",
    "# calculate and save vis score per label \n",
    "vis_stats_df = mp_save_vis_stats_by_label(mp_all_df, vid_in_path, output_parent_folder)\n",
    "\n",
    "# yolo \n",
    "yolo_vis_lineplot(yolo_df, vid_in_path, output_parent_folder)\n",
    "\n",
    "# outputs: save boxplot and lineplot .png and vis_stats_df with mean, standard deviation, and median visibility for each marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e07652-3daf-40e0-9499-308b94f1c3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Segment turn, toward, or away from camera \n",
    "\n",
    "max_gap = 0.12 # max gap to interpolate over \n",
    "cutoff = 0.4\n",
    "order = 1\n",
    "\n",
    "# MEDIAPIPE HIP DATA \n",
    "# Interpolate \n",
    "r_hip_mp_z_interp_df = interpolate_landmark_single_axis(mp_all_df, # mediapipe data frame \n",
    "                                                       'right_hip', # marker to interpolate \n",
    "                                                       'Z_pose', # axis to interpolate \n",
    "                                                       max_gap, # seconds, maximum gap to interpolate over \n",
    "                                                       fps,\n",
    "                                                       vid_in_path, \n",
    "                                                       output_parent_folder,\n",
    "                                                      mediapipe_or_yolo = 'mediapipe')\n",
    "\n",
    "l_hip_mp_z_interp_df = interpolate_landmark_single_axis(mp_all_df, # mediapipe data frame \n",
    "                                                       'left_hip', # marker to interpolate \n",
    "                                                       'Z_pose', # axis to interpolate \n",
    "                                                       max_gap, # seconds, maximum gap to interpolate over \n",
    "                                                       fps,\n",
    "                                                       vid_in_path, \n",
    "                                                       output_parent_folder, \n",
    "                                                      mediapipe_or_yolo = 'mediapipe')\n",
    "\n",
    "# filter \n",
    "r_hip_mp_z_interp_filt = filter_landmark_single_axis(r_hip_mp_z_interp_df.iloc[:, 2],  # one series, position data from one axis of one landmark\n",
    "                                                  fps, # video HZ\n",
    "                                                  cutoff, # filter cutoff \n",
    "                                                  order, # butterworth filter order\n",
    "                                                  vid_in_path,\n",
    "                                                  output_parent_folder\n",
    "                                                  )\n",
    "\n",
    "l_hip_mp_z_interp_filt = filter_landmark_single_axis(l_hip_mp_z_interp_df.iloc[:, 2],  # one series, position data from one axis of one landmark\n",
    "                                                  fps, # video HZ\n",
    "                                                  cutoff, # filter cutoff \n",
    "                                                  order, # butterworth filter order\n",
    "                                                  vid_in_path,\n",
    "                                                  output_parent_folder\n",
    "                                                  )\n",
    "\n",
    "# YOLO HIP DATA \n",
    "# interpolate  \n",
    "r_hip_yolo_z_interp_df = interpolate_landmark_single_axis(yolo_df, # mediapipe data frame \n",
    "                                                       'right_hip', # marker to interpolate \n",
    "                                                       'X_yolo', # axis to interpolate \n",
    "                                                       max_gap, # seconds, maximum gap to interpolate over \n",
    "                                                       fps,\n",
    "                                                       vid_in_path, \n",
    "                                                       output_parent_folder,\n",
    "                                                      mediapipe_or_yolo = 'yolo')\n",
    "\n",
    "l_hip_yolo_z_interp_df = interpolate_landmark_single_axis(yolo_df, # mediapipe data frame \n",
    "                                                       'left_hip', # marker to interpolate \n",
    "                                                       'X_yolo', # axis to interpolate \n",
    "                                                       max_gap, # seconds, maximum gap to interpolate over \n",
    "                                                       fps,\n",
    "                                                       vid_in_path, \n",
    "                                                       output_parent_folder,\n",
    "                                                      mediapipe_or_yolo = 'yolo')\n",
    "\n",
    "# filter \n",
    "\n",
    "\n",
    "# segment code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bd33fd1-93d4-40a4-bd8c-905a45fc8273",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\venv_home_video_analysis_2\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'X'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# this code works - testing alternative above  \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# step 004 - segment when person is walking toward or away from camera or turning \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m [turn_df, walks_df] \u001b[38;5;241m=\u001b[39m \u001b[43msegment_video_walks_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp_all_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvid_in_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_parent_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfind_peaks_distance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# min distance between hip z distance peaks  \u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfind_peaks_prominence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# hip Z peaks need to be greater than this value to count as a turn\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mflattening_point_atol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0025\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Distance of hip z diff away from zero to be identified as \"flattening point\"\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdist_turn_mid_to_flattening\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# flattening point has to be at least this many frames from turn midpoint\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# outputs \u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# turn_df: pandas data frame with frames for each turn start, stop, and midpoint; also saved as .csv \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# walks_df: pandas data frame with frames for each walk start, stop, and midpoint and if walk was toward or away from camera; also saved as .csv \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 2 plots saved as .png with turn/walk start stop and hip and shoulder distances \u001b[39;00m\n",
      "File \u001b[1;32m~\\Box\\MM_Personal\\5_Projects\\BoveLab\\3_Data_and_Code\\hva_code\\HomeVideoAnalysis\\megan_sandbox\\sandbox_main_src_funs\\segment_video_walk_turn.py:137\u001b[0m, in \u001b[0;36msegment_video_walks_turn\u001b[1;34m(mp_all_df, yolo_df, fps, vid_in_path, output_parent_folder, cutoff, order, find_peaks_distance, find_peaks_prominence, flattening_point_atol, dist_turn_mid_to_flattening)\u001b[0m\n\u001b[0;32m    134\u001b[0m hip_l_yolo_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m hip_l_yolo_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# hip width \u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m hip_width_yolo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[43mhip_r_yolo_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m hip_l_yolo_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    138\u001b[0m hip_width_yolo_smooth \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(hip_width_yolo)\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# frames \u001b[39;00m\n",
      "File \u001b[1;32m~\\venv_home_video_analysis_2\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\venv_home_video_analysis_2\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'X'"
     ]
    }
   ],
   "source": [
    "# this code works - testing alternative above  \n",
    "\n",
    "# step 004 - segment when person is walking toward or away from camera or turning \n",
    "[turn_df, walks_df] = segment_video_walks_turn(mp_all_df, yolo_df, fps, vid_in_path, output_parent_folder,\n",
    "                                               cutoff = 0.4, order = 1,\n",
    "                                               find_peaks_distance = 200, # min distance between hip z distance peaks  \n",
    "                                               find_peaks_prominence = 0.2, # hip Z peaks need to be greater than this value to count as a turn\n",
    "                                               flattening_point_atol = 0.0025, # Distance of hip z diff away from zero to be identified as \"flattening point\"\n",
    "                                               dist_turn_mid_to_flattening = 20) # flattening point has to be at least this many frames from turn midpoint\n",
    "\n",
    "# outputs \n",
    "# turn_df: pandas data frame with frames for each turn start, stop, and midpoint; also saved as .csv \n",
    "# walks_df: pandas data frame with frames for each walk start, stop, and midpoint and if walk was toward or away from camera; also saved as .csv \n",
    "# 2 plots saved as .png with turn/walk start stop and hip and shoulder distances "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_home_video_analysis_2",
   "language": "python",
   "name": "venv_home_video_analysis_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
