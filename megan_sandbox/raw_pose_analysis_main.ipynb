{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a878794-1037-4cf6-8c29-3cabd32dd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze raw pose estimation from vertical videos only and get gait metrics\n",
    "\n",
    "# required folder structure of input folder (raw_pose_data_in_path, mirrors video folder structure)\n",
    "# raw_pose_data_in_path\n",
    "    # participant 1\n",
    "        # date \n",
    "            # 000_raw_pose_data (folder containing pose, world, and yolo raw pose data .csv files) \n",
    "    # BW-12 \n",
    "        # 2024-05-02  \n",
    "            # 000_raw_pose_data (folder containing pose, world, and yolo raw pose data .csv files)\n",
    "\n",
    "#  outputs from this analysis per patient will be saved in the same folder \n",
    "\n",
    "# raw_pose_data_in_path\n",
    "    # participant 1\n",
    "        # date \n",
    "            # 001 analysis results \n",
    "            # 002 analysis results \n",
    "\n",
    "# .csv for all participants in raw_pose_data_in_path saved as all_participants_pose_metrics_df variable \n",
    "    # save pose metrics from all videos in raw_pose_data_in_path saved as  \n",
    "    # 'all_participants_pose_metrics_df.csv' in raw_pose_data_in_path folder \n",
    "\n",
    "# note! if more than one video for each task, 'gait_vertical_left_2' and 'gait_vertical_left_3'\n",
    "    # if more then three videos per task, need to update 'task' variable below to include 'gait_vertical_left_3' etc \n",
    "\n",
    "# try for no spaces in folder and file names, but should still work if there are spaces \n",
    "\n",
    "# if running on large dataset, may need to run on chunks of raw pose estimation data then merge .csv files with pose metrices all, included/excluded data, etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceab7caf-76d8-4c08-888f-91ab68c7a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b0c2c0-a0d7-4124-b69d-6b05e0a7d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from sandbox src code files \n",
    "from raw_pose_analysis_funs.merge_mp_yolo_dfs import (merge_mp_pose_world, clean_mp_yolo_missing_data, add_orientation_and_turn_direction, save_merge_mp_yolo_df)\n",
    "from raw_pose_analysis_funs.frames_to_time import (add_time_column, save_df_w_time)\n",
    "from raw_pose_analysis_funs.landmark_visibility import (mp_vis_all_labels_boxplot, mp_vis_lineplot, mp_save_vis_stats_by_label, yolo_vis_lineplot)\n",
    "from raw_pose_analysis_funs.vel_pixel_h_proxy import calc_velocity_proxy\n",
    "from raw_pose_analysis_funs.select_linear_walking import select_plot_linear_walking\n",
    "from raw_pose_analysis_funs.segment_video_walk_turn import (segment_video_interp_filter, segment_video_walks_turn)\n",
    "from raw_pose_analysis_funs.gait_metric_stride_time import(stride_time_interp, calculate_stride_time)\n",
    "from raw_pose_analysis_funs.gait_metric_cadence import (calculate_cadence)\n",
    "from raw_pose_analysis_funs.gait_metric_stride_width import (stride_width_interp, calculate_stride_width)\n",
    "from raw_pose_analysis_funs.gait_metric_support import (support_interp, ankle_y_vel_accel_peak_min, \n",
    "id_toe_off_heel_strike, calculate_single_double_support, create_blank_df_for_no_support)\n",
    "from raw_pose_analysis_funs.gait_metrics_compile_stats import save_all_pose_metrics\n",
    "from raw_pose_analysis_funs.gait_metric_support_2 import (id_calc_support_metrics, calc_support_stats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c572cbf1-ca14-4276-9694-12bef6455d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables manually  \n",
    "\n",
    "# Set input path to run all analysis \n",
    "# folder with all raw pose data (dir_in_path) \n",
    "raw_pose_data_in_path = r\"C:\\Users\\mmccu\\Box\\Brainwalk\\Home Video Walking\\Walking home videos for analysis\\Instruction_V2\\need_to_process\"\n",
    "\n",
    "# input variables for interpolation and filtering \n",
    "max_gap = 0.12 # max gap to interpolate over \n",
    "cutoff = 0.4\n",
    "order = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38fc76f-e30d-41ff-81da-726d61cf24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of all csv files in raw pose data folders \n",
    "\n",
    "raw_data_full_path_all = [] \n",
    "raw_data_file_names_all = []\n",
    "\n",
    "# loop through all files in input path \n",
    "for (dir_path, dir_names, file_names) in os.walk(raw_pose_data_in_path):\n",
    "    for file_name in file_names: \n",
    "        name, ext = os.path.splitext(file_name)\n",
    "        ext = ext.lower()[1:]\n",
    "        current_raw_data_in_path = os.path.join(dir_path, file_name) # full path to files \n",
    "        # save full path to file if it meets requirements to be raw pose data \n",
    "        if (ext == 'csv') & ('000_raw_pose_data' in current_raw_data_in_path): # from run.py, outputs all saved in 000_raw_pose_data_folder\n",
    "            raw_data_full_path_all = raw_data_full_path_all + [current_raw_data_in_path]\n",
    "            raw_data_file_names_all = raw_data_file_names_all + [file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bb3357-aa98-4a97-bb1e-ba57c4393dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data from: BW-0120\\05-15-2025\n",
      "Analyzing video_id_date_name: gait_vertical_left_BW-0120_05-15-2025\n",
      "walk segment number: 0\n",
      "walk segment number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmccu\\venv_home_video_analysis_2\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk segment number: 2\n",
      "walk segment number: 3\n",
      "walk segment number: 4\n",
      "walk segment number: 5\n",
      "Analyzing video_id_date_name: gait_vertical_right_BW-0120_05-15-2025\n",
      "walk segment number: 0\n",
      "walk segment number: 1\n",
      "walk segment number: 2\n",
      "walk segment number: 3\n",
      "walk segment number: 4\n",
      "walk segment number: 5\n",
      "walk segment number: 6\n"
     ]
    }
   ],
   "source": [
    "# find all unique ID date combos (each unique folder with videos)\n",
    "id_date_all = []\n",
    "for file_i, raw_path in enumerate(raw_data_full_path_all): \n",
    "    parent_path_1, current_file_name = os.path.split(raw_path)\n",
    "    parent_path_2, current_raw_data_folder = os.path.split(parent_path_1)\n",
    "    parent_path_3, current_date = os.path.split(parent_path_2)\n",
    "    parent_path_4, current_id = os.path.split(parent_path_3)\n",
    "    current_id_date = os.path.join(current_id, current_date)\n",
    "    id_date_all = id_date_all + [current_id_date]\n",
    "\n",
    "# add if statement to pick specific IDs\n",
    "# if float(current_id[3:]) >= 204:\n",
    "#    current_id_date = os.path.join(current_id, current_date)\n",
    "#    id_date_all = id_date_all + [current_id_date]\n",
    "\n",
    "# for each unique ID and date combo, \n",
    "#select either all gait_vertical_right .csv files or all gait_vertical_left files and run all analysis \n",
    "\n",
    "# all unique ID and date combos of .csv files in raw pose data folder \n",
    "unique_id_date = list(set(id_date_all))\n",
    "unique_id_date = sorted(unique_id_date) # run in same order every time\n",
    "\n",
    "# gait_vertical_left and gait_vertical_right = home videos \n",
    "# PWS, FW, and TUG = in person BW zeno videos \n",
    "tasks = ['gait_vertical_left', \n",
    "         'gait_vertical_left_2', \n",
    "         'gait_vertical_left_3',\n",
    "         'gait_vertical_right', \n",
    "         'gait_vertical_right_2',\n",
    "         'gait_vertical_right_3',\n",
    "         'gait_vertical_PWS_1',\n",
    "         'gait_vertical_PWS_2',\n",
    "         'gait_vertical_FW_1', \n",
    "         'gait_vertical_FW_2',\n",
    "         'gait_vertical_TUG_1',\n",
    "         'gait_vertical_TUG_2'\n",
    "        ] \n",
    "\n",
    "# set blank variables to add file names to be included or excluded \n",
    "included_videos = []\n",
    "excluded_videos = []\n",
    "\n",
    "vids_insufficient_data = []\n",
    "\n",
    "# blank to save all velocity proxy measures \n",
    "delta_pix_h_rel_median_all = []\n",
    "\n",
    "# blank to populate with all included participant metrics \n",
    "all_participants_pose_metrics = []\n",
    "\n",
    "for id_date_i, id_date in enumerate(unique_id_date): \n",
    "    print('Analyzing data from: ' + id_date)\n",
    "    # select raw data paths that match id_date combo \n",
    "    current_id_date_csv_file_paths = [item for item in raw_data_full_path_all if id_date in item]\n",
    "\n",
    "    # set output folder prefix. Find relative paths relative from input folder to raw data .csv \n",
    "    raw_data_relpath = os.path.relpath(current_id_date_csv_file_paths[0], raw_pose_data_in_path)\n",
    "    rel_path_to_date_subfolder, raw_data_subfolder = os.path.split(os.path.dirname(raw_data_relpath))\n",
    "    dir_out_prefix = os.path.normpath(os.path.join(raw_pose_data_in_path, rel_path_to_date_subfolder))\n",
    "   # print('dir_out_prefix: ' + dir_out_prefix)\n",
    "    \n",
    "    # select raw data paths that match right or left vertical task/video\n",
    "    for task_i, task, in enumerate(tasks):  \n",
    "        # select files matching current task \n",
    "        current_task_csv_paths = [item for item in current_id_date_csv_file_paths if task in item]\n",
    "\n",
    "         # populate in later step \n",
    "        valid_segments_found = [] \n",
    "        \n",
    "        # if files with raw data for this task is saved in subfolder, run analysis  \n",
    "        if len(current_task_csv_paths) == 0: \n",
    "            #print('Skipped: no files matching ' + task)\n",
    "            valid_segments_found = []\n",
    "        elif (len(current_task_csv_paths) > 0) & (len(current_task_csv_paths) <= 3):\n",
    "            valid_segments_found = []\n",
    "            vids_insufficient_data.append(current_task_csv_paths[0]) # often some videos with fps but not pose data \n",
    "        else: \n",
    "            # after selecting for correct task, save each raw data path as own variable \n",
    "            current_yolo_data_path = [item for item in current_task_csv_paths if 'yolo.csv' in item]\n",
    "            current_mp_pose_data_path = [item for item in current_task_csv_paths if 'mediapipe.csv' in item]\n",
    "            current_mp_world_data_path = [item for item in current_task_csv_paths if 'mediapipe_world.csv' in item]\n",
    "            current_video_fps_path = [item for item in current_task_csv_paths if 'fps.csv' in item]\n",
    "\n",
    "            # # [-1]: if more than 1 'gait_vertical_right' video, \n",
    "                # make sure to select 'gait_vertical_right' (last path), not 'gait_vertical_right_2' \n",
    "            if len(current_yolo_data_path) > 1: \n",
    "                current_yolo_data_path = [sorted(current_yolo_data_path)[-1]]\n",
    "            if len(current_mp_pose_data_path) > 1:\n",
    "                current_mp_pose_data_path = [sorted(current_mp_pose_data_path)[-1]]\n",
    "            if len(current_mp_world_data_path) > 1:\n",
    "                current_mp_world_data_path = [sorted(current_mp_world_data_path)[-1]]\n",
    "            if len(current_video_fps_path) > 1:\n",
    "                current_video_fps_path = [sorted(current_video_fps_path)[-1]]\n",
    "            \n",
    "            # read in raw pose data, all three types \n",
    "            mp_pose_df = pd.read_csv(current_mp_pose_data_path[0])\n",
    "            mp_world_df = pd.read_csv(current_mp_world_data_path[0])\n",
    "            yolo_df = pd.read_csv(current_yolo_data_path[0])\n",
    "            \n",
    "            # read in frames per seconds and save as var for future code \n",
    "            fps_df = pd.read_csv(current_video_fps_path[0], index_col = 0)\n",
    "            fps = fps_df.iloc[0,0]\n",
    "\n",
    "            # set video ID - ID_date_task (analogous to vid_in_path in run script, use for plot and file names)\n",
    "            yolo_basename = os.path.splitext(os.path.basename(current_yolo_data_path[0]))[0]\n",
    "            video_id_date_name = (yolo_basename).replace('yolo', id_date).replace('\\\\', '_')\n",
    "            print('Analyzing video_id_date_name: ' + video_id_date_name)\n",
    "            \n",
    "            # run analysis functions \n",
    "            # step 001 - merge and clean yolo and mediapipe data \n",
    "            [mp_all_df, yolo_df] = merge_mp_pose_world(mp_pose_df, mp_world_df, yolo_df)\n",
    "            [mp_all_df, yolo_df] = clean_mp_yolo_missing_data(mp_all_df, yolo_df)\n",
    "            [mp_all_df, yolo_df] = add_orientation_and_turn_direction(video_id_date_name, mp_all_df, yolo_df) # check vid_in_path \n",
    "            # save_merge_mp_yolo_df(mp_all_df, yolo_df, video_id_date_name, dir_out_prefix)\n",
    "\n",
    "            # 002 frames to time: save .csv file with seconds in 003 output folder \n",
    "            [mp_all_df, yolo_df] = add_time_column(mp_all_df, yolo_df, fps)\n",
    "            save_df_w_time(mp_all_df, yolo_df, video_id_date_name, dir_out_prefix) \n",
    "\n",
    "            # save var of total video time \n",
    "            current_vid_duration = mp_all_df['time_seconds'].iloc[-1] - mp_all_df['time_seconds'].iloc[0]\n",
    "\n",
    "            # 003 - plot and save landmark visibility scores for yolo and mediapipe \n",
    "            # yolo \n",
    "            yolo_vis_lineplot(yolo_df, video_id_date_name, dir_out_prefix)\n",
    "            # mediapipe \n",
    "            mp_vis_all_labels_boxplot(mp_all_df, video_id_date_name, dir_out_prefix) \n",
    "            mp_vis_lineplot(mp_all_df, video_id_date_name, dir_out_prefix)\n",
    "            # calculate and save mean, standard deviation, and median visibility for each marker\n",
    "            vis_stats_df = mp_save_vis_stats_by_label(mp_all_df, video_id_date_name, dir_out_prefix)\n",
    "\n",
    "            # -------------------------------------\n",
    "            # calculate velocity proxy for each video using yolo hip Y position from original yolo df \n",
    "            vel_output_folder = os.path.join(dir_out_prefix, '005_gait_metrics', 'vel_pixel_proxy')\n",
    "            if not os.path.exists(vel_output_folder):\n",
    "                os.makedirs(vel_output_folder)\n",
    "            \n",
    "            delta_pix_h_rel_median = calc_velocity_proxy(yolo_df, video_id_date_name, vel_output_folder, fps)\n",
    "            \n",
    "            # save all medians to join with all other metrics in next step \n",
    "            delta_pix_h_rel_median_all.append({'id_date_pose' : id_date,\n",
    "                                              'video_id_date_name_pose' : video_id_date_name,\n",
    "                                               'task_pose' : task,\n",
    "                                               'frames_per_second_pose' : fps,\n",
    "                                               'total_video_duration_sec_pose' : current_vid_duration,      \n",
    "                                               'delta_pix_h_rel_median_pose' : delta_pix_h_rel_median})\n",
    "\n",
    "            # ---------------------------------------------\n",
    "            # 3.5  selecting linear walking, remove extraneous activity \n",
    "            valid_segments_all, valid_segments_found = select_plot_linear_walking(mp_all_df, yolo_df, fps, video_id_date_name, dir_out_prefix)\n",
    "            \n",
    "            # if not valid segments found, skip analysis steps and save name to exclude list \n",
    "            if valid_segments_found == 0: \n",
    "                excluded_videos.append(video_id_date_name) \n",
    "            else: # a valid segments are found, calculate metrics over each segment \n",
    "                included_videos.append(video_id_date_name)\n",
    "\n",
    "                #005 calculate gait metrics over each walk segment \n",
    "                stride_time_stats_per_walk = pd.DataFrame()\n",
    "                stride_times_all = pd.DataFrame()\n",
    "\n",
    "                cadence_per_walk = pd.DataFrame()\n",
    "\n",
    "                stride_width_stats_per_walk = pd.DataFrame()\n",
    "                stride_width_all = pd.DataFrame()\n",
    "\n",
    "                support_metric_stats_per_walk = pd.DataFrame()\n",
    "                support_metrics_all = pd.DataFrame() \n",
    "\n",
    "                for segment_i, current_segment in enumerate(valid_segments_all):\n",
    "                    print('walk segment number: ' + str(segment_i))\n",
    "                    start_sec = current_segment['time_seconds'].iloc[0]\n",
    "                    end_sec = current_segment['time_seconds'].iloc[-1]\n",
    "                    \n",
    "                    # select yolo and mediapipe df between end and start seconds \n",
    "                    current_mp_all_df = mp_all_df[(mp_all_df['time_seconds'] >= start_sec) & (mp_all_df['time_seconds'] <= end_sec)]\n",
    "                    current_segment_yolo_df = yolo_df[(yolo_df['time_seconds'] >= start_sec) & (yolo_df['time_seconds'] <= end_sec)]\n",
    "            \n",
    "                    #stride time ------------------------------\n",
    "                    mp_ankle_Y_interp = stride_time_interp(current_mp_all_df, video_id_date_name, dir_out_prefix, max_gap, fps)\n",
    "                    [stride_time_stats_df, stride_times_peaks, stride_times_valleys] = calculate_stride_time(mp_ankle_Y_interp, fps,\n",
    "                                                                                                     video_id_date_name, dir_out_prefix,\n",
    "                                                                                                     rolling_mean_window = round(.5 * fps), \n",
    "                                                                                                     find_peaks_distance = round(.33 * fps), # min distance between ankle y distance peaks (frames) \n",
    "                                                                                                     find_peaks_prominence = 0.01, # ankle y peaks need to be greater than this value to count as step\n",
    "                                                                                                     walk_num = str(segment_i))\n",
    "                    # save stride time stats per walking segment \n",
    "                    stride_time_stats_per_walk = pd.concat([stride_time_stats_per_walk, stride_time_stats_df])\n",
    "                    \n",
    "                    # join times from peaks and valleys \n",
    "                    peak_and_valley_times = pd.concat([pd.DataFrame(stride_times_peaks), pd.DataFrame(stride_times_valleys)]) # all times for one walk\n",
    "                    stride_times_all = pd.concat([stride_times_all, peak_and_valley_times])\n",
    "\n",
    "                    # cadence --------------------------------------\n",
    "                    total_steps, video_length_sec, cadence_df = calculate_cadence(stride_times_peaks, stride_times_valleys,\n",
    "                                                                                    start_sec, end_sec,\n",
    "                                                                                    video_id_date_name, dir_out_prefix)\n",
    "                    # save cadence for each segment \n",
    "                    cadence_per_walk = pd.concat([cadence_per_walk, cadence_df])\n",
    "\n",
    "                    # stride width ---------------------------------\n",
    "                    mp_stride_width_interp_dfs = stride_width_interp(current_mp_all_df, video_id_date_name, dir_out_prefix, max_gap, fps)\n",
    "                    stride_width_stats_df, stride_width = calculate_stride_width(mp_stride_width_interp_dfs, \n",
    "                                                                                 video_id_date_name, \n",
    "                                                                                 dir_out_prefix, \n",
    "                                                                                 walk_num = str(segment_i))\n",
    "                    # save stride width stats per walking segment \n",
    "                    stride_width_stats_per_walk = pd.concat([stride_width_stats_per_walk, stride_width_stats_df])\n",
    "                    \n",
    "                    # save stride widths for all segments \n",
    "                    stride_width_all = pd.concat([stride_width_all, stride_width])\n",
    "\n",
    "                    # single and double support ----------------------------------------\n",
    "                    # if participant is moving toward camera \n",
    "                    if (current_segment['pattern'] == 'increasing').all(): \n",
    "#                        print('increasing - calc support')\n",
    "                        metrics_per_stride_df = id_calc_support_metrics(current_mp_all_df, fps, video_id_date_name, dir_out_prefix, walk_num = str(segment_i))\n",
    "\n",
    "                        # if at least one stride with data, calculate stats and append to per walk \n",
    "                        support_metric_stats_df = calc_support_stats(metrics_per_stride_df)\n",
    "\n",
    "                        # save suport stats per walking segment \n",
    "                        support_metric_stats_per_walk = pd.concat([support_metric_stats_per_walk, support_metric_stats_df]) \n",
    "\n",
    "                        # save support metrics per stride for all segments \n",
    "                        support_metrics_all = pd.concat([support_metrics_all, metrics_per_stride_df]) \n",
    "\n",
    "                # ----------------------------------------------------------------\n",
    "                # save results for each walking segment and for all walking segments combined \n",
    "                vid_in_path_no_ext = os.path.splitext(os.path.basename(video_id_date_name))[0]\n",
    "                \n",
    "                # stride time  \n",
    "                # each row = 1 walk segment \n",
    "                stride_time_output = os.path.join(dir_out_prefix, '005_gait_metrics', 'stride_time')\n",
    "                stride_time_stats_path = os.path.normpath(os.path.join(stride_time_output, (vid_in_path_no_ext + '_stride_time_stats_per_walk.csv')))\n",
    "                stride_time_stats_per_walk.to_csv(stride_time_stats_path)\n",
    "\n",
    "                # save stride time summary stats for all strides in all walking segments \n",
    "                stride_time_stats_all = pd.DataFrame(data = {'stride_time_mean_sec' : [stride_times_all['seconds'].mean(skipna = True)], \n",
    "                                                             'stride_time_median_sec': [stride_times_all['seconds'].median(skipna = True)], \n",
    "                                                             'stride_time_std' : [stride_times_all['seconds'].std(skipna = True)], \n",
    "                                                             'stride_time_cv' : [(stride_times_all['seconds'].std(skipna = True) / \n",
    "                                                                                   stride_times_all['seconds'].mean(skipna = True)) * 100],\n",
    "                                                             'stride_time_max' : [stride_times_all['seconds'].max(skipna = True)],\n",
    "                                                             'stride_time_min' : [stride_times_all['seconds'].min(skipna = True)]\n",
    "                                                            })\n",
    "                stride_time_all_stats_path = os.path.normpath(os.path.join(stride_time_output, (vid_in_path_no_ext + '_stride_time_stats_all.csv')))\n",
    "                stride_time_stats_all.to_csv(stride_time_all_stats_path)\n",
    "\n",
    "                # cadence \n",
    "                cadence_output_folder = os.path.join(dir_out_prefix, '005_gait_metrics', 'cadence')\n",
    "                if not os.path.exists(cadence_output_folder):\n",
    "                    os.makedirs(cadence_output_folder)\n",
    "                \n",
    "                # each row = cadence from 1 walk  \n",
    "                cadence_per_walk_path = os.path.normpath(os.path.join(cadence_output_folder, (vid_in_path_no_ext + '_cadence_per_walk.csv')))\n",
    "                cadence_per_walk.to_csv(cadence_per_walk_path)\n",
    "\n",
    "                # mean of cadence from all walk segments \n",
    "                cadence_mean_df = pd.DataFrame(index = range(1), \n",
    "                                               data = {'mean_cadence_step_per_min' : cadence_per_walk['cadence_step_per_min'].mean(skipna = True)})\n",
    "                cadence_mean_path = os.path.normpath(os.path.join(cadence_output_folder, (vid_in_path_no_ext + '_mean_cadence.csv')))\n",
    "                cadence_mean_df.to_csv(cadence_mean_path)\n",
    "\n",
    "                # stride width \n",
    "                width_output_folder = os.path.join(dir_out_prefix, '005_gait_metrics', 'stride_width')\n",
    "                if not os.path.exists(width_output_folder):\n",
    "                    os.makedirs(width_output_folder)\n",
    "\n",
    "                # save stats = each row = 1 walk \n",
    "                stride_width_stats_path = os.path.normpath(os.path.join(width_output_folder, (vid_in_path_no_ext + '_stride_width_stats_per_walk.csv')))\n",
    "                stride_width_stats_per_walk.to_csv(stride_width_stats_path)\n",
    "\n",
    "                # stats across all walks \n",
    "                stride_width_stats_all = pd.DataFrame(data = {'stride_width_mean_cm' : [stride_width_all['heel_x_diff_0'].mean(skipna = True)],\n",
    "                                                              'stride_width_median_cm' : [stride_width_all['heel_x_diff_0'].median(skipna = True)],\n",
    "                                                              'stride_width_std' : [stride_width_all['heel_x_diff_0'].std(skipna = True)], \n",
    "                                                              'stride_width_cv' : [(stride_width_all['heel_x_diff_0'].std(skipna = True) / \n",
    "                                                                                    stride_width_all['heel_x_diff_0'].mean(skipna = True)) * 100],\n",
    "                                                              'stride_width_max' : [stride_width_all['heel_x_diff_0'].max(skipna = True)],\n",
    "                                                              'stride_width_min' : [stride_width_all['heel_x_diff_0'].min(skipna = True)]\n",
    "                                                              })\n",
    "                \n",
    "                stride_width_all_stats_path = os.path.normpath(os.path.join(width_output_folder, (vid_in_path_no_ext + '_stride_width_stats_all.csv')))\n",
    "                stride_width_stats_all.to_csv(stride_width_all_stats_path)\n",
    "\n",
    "                # double + single support  \n",
    "                support_output_folder = os.path.join(dir_out_prefix, '005_gait_metrics', 'support_v2')\n",
    "                if not os.path.exists(support_output_folder):\n",
    "                    os.makedirs(support_output_folder)\n",
    "\n",
    "                # one row per walk \n",
    "                support_metrics_per_walk_path = os.path.normpath(os.path.join(support_output_folder, (vid_in_path_no_ext + '_support_metrics_stats_per_walk.csv')))\n",
    "                support_metric_stats_per_walk.to_csv(support_metrics_per_walk_path)\n",
    "\n",
    "                # support metrics stats for all strides in the trial \n",
    "                support_metrics_stats_all = calc_support_stats(support_metrics_all)\n",
    "                \n",
    "                support_metrics_all_stats_path =  os.path.normpath(os.path.join(support_output_folder, (vid_in_path_no_ext + '_support_metrics_stats_all.csv')))\n",
    "                support_metrics_stats_all.to_csv(support_metrics_all_stats_path)\n",
    "                    \n",
    "        # if valid segments found, compile all metrics stats for one participant into one .csv and save for all participants df \n",
    "        if valid_segments_found == 1:\n",
    "            pose_metrics_df = save_all_pose_metrics(video_id_date_name,\n",
    "                                                    valid_segments_all,\n",
    "                                                    stride_time_stats_all, \n",
    "                                                    cadence_mean_df, \n",
    "                                                    stride_time_stats_df, \n",
    "                                                    stride_width_stats_all,\n",
    "                                                    support_metrics_stats_all,\n",
    "                                                    dir_out_prefix)\n",
    "\n",
    "            all_participants_pose_metrics.append(pose_metrics_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f06093b-205d-492e-9f6c-fc304e1b83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_parent, all_gait_metrics_filename = os.path.split(raw_pose_data_in_path)\n",
    "\n",
    "# save file on included vs excluded walks - gait metric calculations (stride time, cadence, etc) \n",
    "excluded_videos_df = pd.DataFrame(data = {'excluded_vids' : excluded_videos})\n",
    "excluded_videos_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_excluded_videos.csv')))\n",
    "\n",
    "included_videos_df = pd.DataFrame(data = {'included_vids' : included_videos})\n",
    "included_videos_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_included_videos.csv')))\n",
    "\n",
    "# save list of videos without enough .csv input files for analysis (often have fps but missing pose data) \n",
    "vids_insufficient_data_df = pd.DataFrame(data = {'vids_insufficient_data' : vids_insufficient_data})\n",
    "vids_insufficient_data_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_insufficient_data_for_gait_analysis.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde68611-29b3-4242-9c71-d4f7f64acdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>walking_segmets_n_pose</th>\n",
       "      <th>walking_segments_duration_mean_pose</th>\n",
       "      <th>walking_segments_duration_median_pose</th>\n",
       "      <th>stride_time_mean_sec_pose</th>\n",
       "      <th>stride_time_median_sec_pose</th>\n",
       "      <th>stride_time_std_pose</th>\n",
       "      <th>stride_time_cv_pose</th>\n",
       "      <th>stride_time_max_pose</th>\n",
       "      <th>stride_time_min_pose</th>\n",
       "      <th>mean_cadence_step_per_min_pose</th>\n",
       "      <th>...</th>\n",
       "      <th>singlesupport_per_std_pose</th>\n",
       "      <th>singlesupport_time_sec_std_pose</th>\n",
       "      <th>stance_time_per_std_pose</th>\n",
       "      <th>stance_time_sec_std_pose</th>\n",
       "      <th>swing_time_per_std_pose</th>\n",
       "      <th>swing_time_sec_std_pose</th>\n",
       "      <th>term_dsupport_sec_std_pose</th>\n",
       "      <th>tot_dsupport_per_std_pose</th>\n",
       "      <th>tot_dsupport_time_sec_std_pose</th>\n",
       "      <th>y_cross_row_index_std_pose</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_id_date_name_pose</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gait_vertical_left_BW-0120_05-15-2025</th>\n",
       "      <td>6</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.23</td>\n",
       "      <td>1.226</td>\n",
       "      <td>1.167</td>\n",
       "      <td>0.177</td>\n",
       "      <td>14.400</td>\n",
       "      <td>1.667</td>\n",
       "      <td>1.033</td>\n",
       "      <td>104.645</td>\n",
       "      <td>...</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.10</td>\n",
       "      <td>11.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11.52</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>11.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gait_vertical_right_BW-0120_05-15-2025</th>\n",
       "      <td>7</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1.132</td>\n",
       "      <td>1.167</td>\n",
       "      <td>0.143</td>\n",
       "      <td>12.669</td>\n",
       "      <td>1.300</td>\n",
       "      <td>0.667</td>\n",
       "      <td>109.530</td>\n",
       "      <td>...</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7.65</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        walking_segmets_n_pose  \\\n",
       "video_id_date_name_pose                                          \n",
       "gait_vertical_left_BW-0120_05-15-2025                        6   \n",
       "gait_vertical_right_BW-0120_05-15-2025                       7   \n",
       "\n",
       "                                        walking_segments_duration_mean_pose  \\\n",
       "video_id_date_name_pose                                                       \n",
       "gait_vertical_left_BW-0120_05-15-2025                                  3.55   \n",
       "gait_vertical_right_BW-0120_05-15-2025                                 3.14   \n",
       "\n",
       "                                        walking_segments_duration_median_pose  \\\n",
       "video_id_date_name_pose                                                         \n",
       "gait_vertical_left_BW-0120_05-15-2025                                    3.23   \n",
       "gait_vertical_right_BW-0120_05-15-2025                                   3.03   \n",
       "\n",
       "                                        stride_time_mean_sec_pose  \\\n",
       "video_id_date_name_pose                                             \n",
       "gait_vertical_left_BW-0120_05-15-2025                       1.226   \n",
       "gait_vertical_right_BW-0120_05-15-2025                      1.132   \n",
       "\n",
       "                                        stride_time_median_sec_pose  \\\n",
       "video_id_date_name_pose                                               \n",
       "gait_vertical_left_BW-0120_05-15-2025                         1.167   \n",
       "gait_vertical_right_BW-0120_05-15-2025                        1.167   \n",
       "\n",
       "                                        stride_time_std_pose  \\\n",
       "video_id_date_name_pose                                        \n",
       "gait_vertical_left_BW-0120_05-15-2025                  0.177   \n",
       "gait_vertical_right_BW-0120_05-15-2025                 0.143   \n",
       "\n",
       "                                        stride_time_cv_pose  \\\n",
       "video_id_date_name_pose                                       \n",
       "gait_vertical_left_BW-0120_05-15-2025                14.400   \n",
       "gait_vertical_right_BW-0120_05-15-2025               12.669   \n",
       "\n",
       "                                        stride_time_max_pose  \\\n",
       "video_id_date_name_pose                                        \n",
       "gait_vertical_left_BW-0120_05-15-2025                  1.667   \n",
       "gait_vertical_right_BW-0120_05-15-2025                 1.300   \n",
       "\n",
       "                                        stride_time_min_pose  \\\n",
       "video_id_date_name_pose                                        \n",
       "gait_vertical_left_BW-0120_05-15-2025                  1.033   \n",
       "gait_vertical_right_BW-0120_05-15-2025                 0.667   \n",
       "\n",
       "                                        mean_cadence_step_per_min_pose  ...  \\\n",
       "video_id_date_name_pose                                                 ...   \n",
       "gait_vertical_left_BW-0120_05-15-2025                          104.645  ...   \n",
       "gait_vertical_right_BW-0120_05-15-2025                         109.530  ...   \n",
       "\n",
       "                                        singlesupport_per_std_pose  \\\n",
       "video_id_date_name_pose                                              \n",
       "gait_vertical_left_BW-0120_05-15-2025                         8.22   \n",
       "gait_vertical_right_BW-0120_05-15-2025                        5.69   \n",
       "\n",
       "                                        singlesupport_time_sec_std_pose  \\\n",
       "video_id_date_name_pose                                                   \n",
       "gait_vertical_left_BW-0120_05-15-2025                              0.10   \n",
       "gait_vertical_right_BW-0120_05-15-2025                             0.06   \n",
       "\n",
       "                                        stance_time_per_std_pose  \\\n",
       "video_id_date_name_pose                                            \n",
       "gait_vertical_left_BW-0120_05-15-2025                      11.52   \n",
       "gait_vertical_right_BW-0120_05-15-2025                      2.22   \n",
       "\n",
       "                                        stance_time_sec_std_pose  \\\n",
       "video_id_date_name_pose                                            \n",
       "gait_vertical_left_BW-0120_05-15-2025                       0.19   \n",
       "gait_vertical_right_BW-0120_05-15-2025                      0.08   \n",
       "\n",
       "                                        swing_time_per_std_pose  \\\n",
       "video_id_date_name_pose                                           \n",
       "gait_vertical_left_BW-0120_05-15-2025                     11.52   \n",
       "gait_vertical_right_BW-0120_05-15-2025                     2.22   \n",
       "\n",
       "                                        swing_time_sec_std_pose  \\\n",
       "video_id_date_name_pose                                           \n",
       "gait_vertical_left_BW-0120_05-15-2025                      0.12   \n",
       "gait_vertical_right_BW-0120_05-15-2025                     0.03   \n",
       "\n",
       "                                        term_dsupport_sec_std_pose  \\\n",
       "video_id_date_name_pose                                              \n",
       "gait_vertical_left_BW-0120_05-15-2025                         0.12   \n",
       "gait_vertical_right_BW-0120_05-15-2025                        0.09   \n",
       "\n",
       "                                        tot_dsupport_per_std_pose  \\\n",
       "video_id_date_name_pose                                             \n",
       "gait_vertical_left_BW-0120_05-15-2025                       11.17   \n",
       "gait_vertical_right_BW-0120_05-15-2025                       7.65   \n",
       "\n",
       "                                        tot_dsupport_time_sec_std_pose  \\\n",
       "video_id_date_name_pose                                                  \n",
       "gait_vertical_left_BW-0120_05-15-2025                             0.16   \n",
       "gait_vertical_right_BW-0120_05-15-2025                            0.10   \n",
       "\n",
       "                                        y_cross_row_index_std_pose  \n",
       "video_id_date_name_pose                                             \n",
       "gait_vertical_left_BW-0120_05-15-2025                         1.27  \n",
       "gait_vertical_right_BW-0120_05-15-2025                        0.75  \n",
       "\n",
       "[2 rows x 85 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all participants with gait metrics - stride time, cadence, width, and support \n",
    "all_participants_pose_metrics_df = pd.concat(all_participants_pose_metrics, ignore_index = True)\n",
    "all_participants_pose_metrics_df = all_participants_pose_metrics_df.set_index('video_id_date_name_pose')\n",
    "all_participants_pose_metrics_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_gait_metrics_only.csv')))\n",
    "\n",
    "all_participants_pose_metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf128b65-41ca-4237-80f9-3b4a3bb5419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all participants with veloicty proxy using change in pixel position  \n",
    "delta_pix_h_rel_median_all_df = pd.DataFrame(delta_pix_h_rel_median_all)\n",
    "delta_pix_h_rel_median_all_df = delta_pix_h_rel_median_all_df.set_index('video_id_date_name_pose')\n",
    "delta_pix_h_rel_median_all_df.head()\n",
    "\n",
    "delta_pix_h_rel_median_all_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_vel_proxy_only.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "036d45c0-95cd-409c-894c-600457e41148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving C:\\Users\\mmccu\\Box\\Brainwalk\\Home Video Walking\\Walking home videos for analysis\\Instruction_V2\\need_to_process\\need_to_process_pose_metrics_all.csv\n"
     ]
    }
   ],
   "source": [
    "# gait metrics and velocity proxy measures into one dataframe \n",
    "joined_df = delta_pix_h_rel_median_all_df.join(all_participants_pose_metrics_df, \n",
    "                                                  on = 'video_id_date_name_pose', \n",
    "                                                  how = 'left')\n",
    "\n",
    "joined_df = joined_df.reset_index()\n",
    "\n",
    "joined_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_pose_metrics_all.csv')))\n",
    "print('Saving ' +  os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_pose_metrics_all.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020dcfa1-73cc-44a8-8a7f-94ae7d388673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vids_insufficient_data_df\n",
      "0\n",
      "number of excluded_videos_df\n",
      "0\n",
      "number of included_videos_df\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print('number of vids_insufficient_data_df')\n",
    "print(len(vids_insufficient_data_df))\n",
    "\n",
    "print('number of excluded_videos_df')\n",
    "print(len(excluded_videos_df))\n",
    "\n",
    "print('number of included_videos_df')\n",
    "print(len(included_videos_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_home_video_analysis_2",
   "language": "python",
   "name": "venv_home_video_analysis_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
