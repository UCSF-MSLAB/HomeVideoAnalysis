{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a878794-1037-4cf6-8c29-3cabd32dd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze raw pose estimation from vertical videos only and get gait metrics\n",
    "\n",
    "# required folder structure of input folder (raw_pose_data_in_path, mirrors video folder structure)\n",
    "# raw_pose_data_in_path\n",
    "    # participant 1\n",
    "        # date \n",
    "            # 000_raw_pose_data (folder containing pose, world, and yolo raw pose data .csv files) \n",
    "    # BW-12 \n",
    "        # 2024-05-02  \n",
    "            # 000_raw_pose_data (folder containing pose, world, and yolo raw pose data .csv files)\n",
    "\n",
    "#  outputs from this analysis per patient will be saved in the same folder \n",
    "\n",
    "# raw_pose_data_in_path\n",
    "    # participant 1\n",
    "        # date \n",
    "            # 001 analysis results \n",
    "            # 002 analysis results \n",
    "\n",
    "# .csv for all participants in raw_pose_data_in_path saved as all_participants_pose_metrics_df variable \n",
    "    # save pose metrics from all videos in raw_pose_data_in_path saved as  \n",
    "    # 'all_participants_pose_metrics_df.csv' in raw_pose_data_in_path folder \n",
    "\n",
    "# note! if more than one video for each task, 'gait_vertical_left_2' and 'gait_vertical_left_3'\n",
    "    # if more then three videos per task, need to update 'task' variable below to include 'gait_vertical_left_3' etc \n",
    "\n",
    "# try for no spaces in folder and file names, but should still work if there are spaces \n",
    "\n",
    "# if running on large dataset, may need to run on chunks of raw pose estimation data then merge .csv files with pose metrices all, included/excluded data, etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceab7caf-76d8-4c08-888f-91ab68c7a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b0c2c0-a0d7-4124-b69d-6b05e0a7d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from sandbox src code files \n",
    "from raw_pose_analysis_funs.merge_mp_yolo_dfs import (merge_mp_pose_world, clean_mp_yolo_missing_data, add_orientation_and_turn_direction, save_merge_mp_yolo_df)\n",
    "from raw_pose_analysis_funs.frames_to_time import (add_time_column, save_df_w_time)\n",
    "from raw_pose_analysis_funs.landmark_visibility import (mp_vis_all_labels_boxplot, mp_vis_lineplot, mp_save_vis_stats_by_label, yolo_vis_lineplot)\n",
    "from raw_pose_analysis_funs.select_linear_walking import select_plot_linear_walking\n",
    "from raw_pose_analysis_funs.segment_video_walk_turn import (segment_video_interp_filter, segment_video_walks_turn)\n",
    "from raw_pose_analysis_funs.gait_metric_stride_time import(stride_time_interp, calculate_stride_time)\n",
    "from raw_pose_analysis_funs.gait_metric_cadence import (calculate_cadence)\n",
    "from raw_pose_analysis_funs.gait_metric_stride_width import (stride_width_interp, calculate_stride_width)\n",
    "from raw_pose_analysis_funs.gait_metric_support import (support_interp, ankle_y_vel_accel_peak_min, \n",
    "id_toe_off_heel_strike, calculate_single_double_support, create_blank_df_for_no_support)\n",
    "from raw_pose_analysis_funs.gait_metrics_compile_stats import save_all_pose_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c572cbf1-ca14-4276-9694-12bef6455d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables manually  \n",
    "\n",
    "# Set input path to run all analysis \n",
    "# folder with all raw pose data (dir_in_path) \n",
    "raw_pose_data_in_path = r'C:\\Users\\mmccu\\Box\\Brainwalk\\Home Video Walking\\Megan Project\\Home Video Instruction Updates\\test_example_vid_pose'\n",
    "\n",
    "# input variables for interpolation and filtering \n",
    "max_gap = 0.12 # max gap to interpolate over \n",
    "cutoff = 0.4\n",
    "order = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38fc76f-e30d-41ff-81da-726d61cf24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of all csv files in raw pose data folders \n",
    "\n",
    "raw_data_full_path_all = [] \n",
    "raw_data_file_names_all = []\n",
    "\n",
    "# loop through all files in input path \n",
    "for (dir_path, dir_names, file_names) in os.walk(raw_pose_data_in_path):\n",
    "    for file_name in file_names: \n",
    "        name, ext = os.path.splitext(file_name)\n",
    "        ext = ext.lower()[1:]\n",
    "        current_raw_data_in_path = os.path.join(dir_path, file_name) # full path to files \n",
    "        # save full path to file if it meets requirements to be raw pose data \n",
    "        if (ext == 'csv') & ('000_raw_pose_data' in current_raw_data_in_path): # from run.py, outputs all saved in 000_raw_pose_data_folder\n",
    "            raw_data_full_path_all = raw_data_full_path_all + [current_raw_data_in_path]\n",
    "            raw_data_file_names_all = raw_data_file_names_all + [file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162e5f85-a1f2-49f3-b395-8f7e83626308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all unique ID date combos (each unique folder with videos)\n",
    "id_date_all = []\n",
    "for file_i, raw_path in enumerate(raw_data_full_path_all): \n",
    "    parent_path_1, current_file_name = os.path.split(raw_path)\n",
    "    parent_path_2, current_raw_data_folder = os.path.split(parent_path_1)\n",
    "    parent_path_3, current_date = os.path.split(parent_path_2)\n",
    "    parent_path_4, current_id = os.path.split(parent_path_3)\n",
    "    current_id_date = os.path.join(current_id, current_date)\n",
    "    id_date_all = id_date_all + [current_id_date]\n",
    "\n",
    "    # add if statement to pick specific IDs\n",
    "    # if float(current_id[3:]) >= 213:\n",
    "        #current_id_date = os.path.join(current_id, current_date)\n",
    "        # id_date_all = id_date_all + [current_id_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb859306-38f9-4da7-8080-0d57f3bd6536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmccu\\venv_home_video_analysis_2\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\mmccu\\\\Box\\\\Brainwalk\\\\Home Video Walking\\\\Megan Project\\\\Home Video Instruction Updates\\\\test_example_vid_pose\\\\2024-11-22\\\\005_gait_metrics\\\\stride_width\\\\too_close_to_camera_gait_vertical_left_2_test_example_vid_pose_2024-11-22_stride_width_stats_per_walk.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 258\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# save stats = each row = 1 walk \u001b[39;00m\n\u001b[0;32m    257\u001b[0m stride_width_stats_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(width_output_folder, (vid_in_path_no_ext \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_stride_width_stats_per_walk.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m--> 258\u001b[0m \u001b[43mstride_width_stats_per_walk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstride_width_stats_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# stats across all walks \u001b[39;00m\n\u001b[0;32m    261\u001b[0m stride_width_stats_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride_width_mean_cm\u001b[39m\u001b[38;5;124m'\u001b[39m : [stride_width_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheel_x_diff_0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(skipna \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)],\n\u001b[0;32m    262\u001b[0m                                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride_width_median_cm\u001b[39m\u001b[38;5;124m'\u001b[39m : [stride_width_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheel_x_diff_0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmedian(skipna \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)],\n\u001b[0;32m    263\u001b[0m                                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride_width_std\u001b[39m\u001b[38;5;124m'\u001b[39m : [stride_width_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheel_x_diff_0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstd(skipna \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)], \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m                                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride_width_min\u001b[39m\u001b[38;5;124m'\u001b[39m : [stride_width_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheel_x_diff_0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(skipna \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[0;32m    268\u001b[0m                                               })\n",
      "File \u001b[1;32m~\\venv_home_video_analysis_2\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\venv_home_video_analysis_2\\lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\venv_home_video_analysis_2\\lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\venv_home_video_analysis_2\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\venv_home_video_analysis_2\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\mmccu\\\\Box\\\\Brainwalk\\\\Home Video Walking\\\\Megan Project\\\\Home Video Instruction Updates\\\\test_example_vid_pose\\\\2024-11-22\\\\005_gait_metrics\\\\stride_width\\\\too_close_to_camera_gait_vertical_left_2_test_example_vid_pose_2024-11-22_stride_width_stats_per_walk.csv'"
     ]
    }
   ],
   "source": [
    "# for each unique ID and date combo, \n",
    "#select either all gait_vertical_right .csv files or all gait_vertical_left files and run all analysis \n",
    "\n",
    "# all unique ID and date combos of .csv files in raw pose data folder \n",
    "unique_id_date = list(set(id_date_all))\n",
    "unique_id_date = sorted(unique_id_date) # run in same order every time\n",
    "\n",
    "# gait_vertical_left and gait_vertical_right = home videos \n",
    "# PWS, FW, and TUG = in person BW zeno videos \n",
    "tasks = ['gait_vertical_left', \n",
    "         'gait_vertical_left_2', \n",
    "         'gait_vertical_left_3',\n",
    "         'gait_vertical_right', \n",
    "         'gait_vertical_right_2',\n",
    "         'gait_vertical_right_3',\n",
    "         'gait_vertical_PWS_1',\n",
    "         'gait_vertical_PWS_2',\n",
    "         'gait_vertical_FW_1', \n",
    "         'gait_vertical_FW_2',\n",
    "         'gait_vertical_TUG_1',\n",
    "         'gait_vertical_TUG_2'\n",
    "        ] \n",
    "\n",
    "# set blank variables to add file names to be included or excluded \n",
    "included_videos = []\n",
    "excluded_videos = []\n",
    "\n",
    "vids_support_not_calculated = []\n",
    "vids_support_calculated = []\n",
    "\n",
    "vids_insufficient_data = []\n",
    "\n",
    "# blank to populate with all included participant metrics \n",
    "all_participants_pose_metrics = []\n",
    "\n",
    "for id_date_i, id_date in enumerate(unique_id_date): \n",
    "  #  print('Analyzing data from: ' + id_date)\n",
    "    # select raw data paths that match id_date combo \n",
    "    current_id_date_csv_file_paths = [item for item in raw_data_full_path_all if id_date in item]\n",
    "\n",
    "    # set output folder prefix. Find relative paths relative from input folder to raw data .csv \n",
    "    raw_data_relpath = os.path.relpath(current_id_date_csv_file_paths[0], raw_pose_data_in_path)\n",
    "    rel_path_to_date_subfolder, raw_data_subfolder = os.path.split(os.path.dirname(raw_data_relpath))\n",
    "    dir_out_prefix = os.path.normpath(os.path.join(raw_pose_data_in_path, rel_path_to_date_subfolder))\n",
    "   # print('dir_out_prefix: ' + dir_out_prefix)\n",
    "    \n",
    "    # select raw data paths that match right or left vertical task/video\n",
    "    for task_i, task, in enumerate(tasks):  \n",
    "        # select files matching current task \n",
    "        current_task_csv_paths = [item for item in current_id_date_csv_file_paths if task in item]\n",
    "\n",
    "         # populate in later step \n",
    "        valid_segments_found = [] \n",
    "        \n",
    "        # if files with raw data for this task is saved in subfolder, run analysis  \n",
    "        if len(current_task_csv_paths) == 0: \n",
    "            #print('Skipped: no files matching ' + task)\n",
    "            valid_segments_found = []\n",
    "        elif (len(current_task_csv_paths) > 0) & (len(current_task_csv_paths) <= 3):\n",
    "            valid_segments_found = []\n",
    "            vids_insufficient_data.append(current_task_csv_paths[0]) # often some videos with fps but not pose data \n",
    "        else: \n",
    "            # after selecting for correct task, save each raw data path as own variable \n",
    "            current_yolo_data_path = [item for item in current_task_csv_paths if 'yolo.csv' in item]\n",
    "            current_mp_pose_data_path = [item for item in current_task_csv_paths if 'mediapipe.csv' in item]\n",
    "            current_mp_world_data_path = [item for item in current_task_csv_paths if 'mediapipe_world.csv' in item]\n",
    "            current_video_fps_path = [item for item in current_task_csv_paths if 'fps.csv' in item]\n",
    "\n",
    "            # # [-1]: if more than 1 'gait_vertical_right' video, \n",
    "                # make sure to select 'gait_vertical_right' (last path), not 'gait_vertical_right_2' \n",
    "            if len(current_yolo_data_path) > 1: \n",
    "                current_yolo_data_path = [sorted(current_yolo_data_path)[-1]]\n",
    "            if len(current_mp_pose_data_path) > 1:\n",
    "                current_mp_pose_data_path = [sorted(current_mp_pose_data_path)[-1]]\n",
    "            if len(current_mp_world_data_path) > 1:\n",
    "                current_mp_world_data_path = [sorted(current_mp_world_data_path)[-1]]\n",
    "            if len(current_video_fps_path) > 1:\n",
    "                current_video_fps_path = [sorted(current_video_fps_path)[-1]]\n",
    "            \n",
    "            # read in raw pose data, all three types \n",
    "            mp_pose_df = pd.read_csv(current_mp_pose_data_path[0])\n",
    "            mp_world_df = pd.read_csv(current_mp_world_data_path[0])\n",
    "            yolo_df = pd.read_csv(current_yolo_data_path[0])\n",
    "            \n",
    "            # read in frames per seconds and save as var for future code \n",
    "            fps_df = pd.read_csv(current_video_fps_path[0], index_col = 0)\n",
    "            fps = fps_df.iloc[0,0]\n",
    "\n",
    "            # set video ID - ID_date_task (analogous to vid_in_path in run script, use for plot and file names)\n",
    "            yolo_basename = os.path.splitext(os.path.basename(current_yolo_data_path[0]))[0]\n",
    "            video_id_date_name = (yolo_basename).replace('yolo', id_date).replace('\\\\', '_')\n",
    "          #  print('Analyzing video_id_date_name: ' + video_id_date_name)\n",
    "        \n",
    "            # run analysis functions \n",
    "            # step 001 - save updated .csv files in 001 output folder \n",
    "            [mp_all_df, yolo_df] = merge_mp_pose_world(mp_pose_df, mp_world_df, yolo_df)\n",
    "            [mp_all_df, yolo_df] = clean_mp_yolo_missing_data(mp_all_df, yolo_df)\n",
    "            [mp_all_df, yolo_df] = add_orientation_and_turn_direction(video_id_date_name, mp_all_df, yolo_df) # check vid_in_path \n",
    "            save_merge_mp_yolo_df(mp_all_df, yolo_df, video_id_date_name, dir_out_prefix)\n",
    "\n",
    "            # 002 frames to time: save .csv file with seconds in 003 output folder \n",
    "            [mp_all_df, yolo_df] = add_time_column(mp_all_df, yolo_df, fps)\n",
    "            save_df_w_time(mp_all_df, yolo_df, video_id_date_name, dir_out_prefix)\n",
    "\n",
    "            # 003 - plot and save landmark visibility scores for yolo and mediapipe \n",
    "            # yolo \n",
    "            yolo_vis_lineplot(yolo_df, video_id_date_name, dir_out_prefix)\n",
    "            # mediapipe \n",
    "            mp_vis_all_labels_boxplot(mp_all_df, video_id_date_name, dir_out_prefix) \n",
    "            mp_vis_lineplot(mp_all_df, video_id_date_name, dir_out_prefix)\n",
    "            # calculate and save mean, standard deviation, and median visibility for each marker\n",
    "            vis_stats_df = mp_save_vis_stats_by_label(mp_all_df, video_id_date_name, dir_out_prefix)\n",
    "\n",
    "            # -------------------------------------\n",
    "            # 3.5 test selecting linear walking, remove extraneous activity \n",
    "            valid_segments_all, valid_segments_found = select_plot_linear_walking(mp_all_df, yolo_df, fps, video_id_date_name, dir_out_prefix)\n",
    "            \n",
    "            # if not valid segments found, skip analysis steps and save name to exclude list \n",
    "            if valid_segments_found == 0: \n",
    "                excluded_videos.append(video_id_date_name) \n",
    "            else: # a valid segments are found, calculate metrics over each segment \n",
    "                included_videos.append(video_id_date_name)\n",
    "\n",
    "                #005 calculate gait metrics over each walk segment \n",
    "                stride_time_stats_per_walk = pd.DataFrame()\n",
    "                stride_times_all = pd.DataFrame()\n",
    "\n",
    "                cadence_per_walk = pd.DataFrame()\n",
    "\n",
    "                stride_width_stats_per_walk = pd.DataFrame()\n",
    "                stride_width_all = pd.DataFrame()\n",
    "\n",
    "                support_for_any_segments = []\n",
    "                support_metrics_per_walk = pd.DataFrame()\n",
    "\n",
    "                for segment_i, current_segment in enumerate(valid_segments_all):\n",
    "                    start_sec = current_segment['time_seconds'].iloc[0]\n",
    "                    end_sec = current_segment['time_seconds'].iloc[-1]\n",
    "                    \n",
    "                    # select yolo and mediapipe df between end and start seconds \n",
    "                    current_mp_all_df = mp_all_df[(mp_all_df['time_seconds'] >= start_sec) & (mp_all_df['time_seconds'] <= end_sec)]\n",
    "                    current_segment_yolo_df = yolo_df[(yolo_df['time_seconds'] >= start_sec) & (yolo_df['time_seconds'] <= end_sec)]\n",
    "            \n",
    "                    #stride time ------------------------------\n",
    "                    mp_ankle_Y_interp = stride_time_interp(current_mp_all_df, video_id_date_name, dir_out_prefix, max_gap, fps)\n",
    "                    [stride_time_stats_df, stride_times_peaks, stride_times_valleys] = calculate_stride_time(mp_ankle_Y_interp, fps,\n",
    "                                                                                                     video_id_date_name, dir_out_prefix,\n",
    "                                                                                                     rolling_mean_window = round(.5 * fps), \n",
    "                                                                                                     find_peaks_distance = round(.33 * fps), # min distance between ankle y distance peaks (frames) \n",
    "                                                                                                     find_peaks_prominence = 0.01, # ankle y peaks need to be greater than this value to count as step\n",
    "                                                                                                     walk_num = str(segment_i))\n",
    "                    # save stride time stats per walking segment \n",
    "                    stride_time_stats_per_walk = pd.concat([stride_time_stats_per_walk, stride_time_stats_df])\n",
    "                    \n",
    "                    # join times from peaks and valleys \n",
    "                    peak_and_valley_times = pd.concat([pd.DataFrame(stride_times_peaks), pd.DataFrame(stride_times_valleys)]) # all times for one walk\n",
    "                    stride_times_all = pd.concat([stride_times_all, peak_and_valley_times])\n",
    "\n",
    "                    # cadence --------------------------------------\n",
    "                    total_steps, video_length_sec, cadence_df = calculate_cadence(stride_times_peaks, stride_times_valleys,\n",
    "                                                                                    start_sec, end_sec,\n",
    "                                                                                    video_id_date_name, dir_out_prefix)\n",
    "                    # save cadence for each segment \n",
    "                    cadence_per_walk = pd.concat([cadence_per_walk, cadence_df])\n",
    "\n",
    "                    # stride width ---------------------------------\n",
    "                    mp_stride_width_interp_dfs = stride_width_interp(current_mp_all_df, video_id_date_name, dir_out_prefix, max_gap, fps)\n",
    "                    stride_width_stats_df, stride_width = calculate_stride_width(mp_stride_width_interp_dfs, \n",
    "                                                                                 video_id_date_name, \n",
    "                                                                                 dir_out_prefix, \n",
    "                                                                                 walk_num = str(segment_i))\n",
    "                    # save stride width stats per walking segment \n",
    "                    stride_width_stats_per_walk = pd.concat([stride_width_stats_per_walk, stride_width_stats_df])\n",
    "                    \n",
    "                    # save stride widths for all segments \n",
    "                    stride_width_all = pd.concat([stride_width_all, stride_width])\n",
    "\n",
    "                    # single and double support - only on segments walking away \n",
    "                    if (current_segment['pattern'] == 'decreasing').all():\n",
    "\n",
    "                        # select and interpolate ankle y data \n",
    "                        yolo_support_interp_dfs = support_interp(current_segment_yolo_df, video_id_date_name, dir_out_prefix, max_gap, fps)\n",
    "                        right_ankle_y = yolo_support_interp_dfs[0]\n",
    "                        left_ankle_y = yolo_support_interp_dfs[1]\n",
    "\n",
    "                        # find peaks and min of velocity and acceleration \n",
    "                        right_ank_y_data = ankle_y_vel_accel_peak_min(right_ankle_y,\n",
    "                                                                      diff_period = round(.167 * fps),  # frames to take diff between for vel + accel\n",
    "                                                                      peaks_distance = round(.167 * fps), # min distance between peaks + valleys \n",
    "                                                                      peaks_prominence_percent_max = .05, # percent of biggest peak that each peak must be greater than to be identified \n",
    "                                                                      valleys_prominence_percent_max = .10) # percent of biggest peak that each valley must be greater than to be identified \n",
    "                        \n",
    "                        left_ank_y_data = ankle_y_vel_accel_peak_min(left_ankle_y, \n",
    "                                                                     diff_period = round(.167 * fps),  # frames to take diff between for vel + accel\n",
    "                                                                     peaks_distance = round(.167 * fps), # min distance between peaks + valleys\n",
    "                                                                     peaks_prominence_percent_max = .05, \n",
    "                                                                     valleys_prominence_percent_max = .10)\n",
    "                        \n",
    "                        # get gait events from position, velocity, and acceleration data\n",
    "                        toe_off_heel_strike_df, enough_data_for_support = id_toe_off_heel_strike(right_ank_y_data, \n",
    "                                                                        left_ank_y_data, \n",
    "                                                                        video_id_date_name, \n",
    "                                                                        dir_out_prefix, \n",
    "                                                                        walk_num = str(segment_i))\n",
    " \n",
    "                        support_for_any_segments.append(enough_data_for_support)\n",
    "                        \n",
    "                        if enough_data_for_support == 1: \n",
    "                            # calculate single and double support from toe off and heel strike frames \n",
    "                            support_metrics_df = calculate_single_double_support(toe_off_heel_strike_df, fps, video_id_date_name, dir_out_prefix, segment_i)\n",
    "                            # one row = one walk segment \n",
    "                            support_metrics_per_walk = pd.concat([support_metrics_per_walk, support_metrics_df])\n",
    "                \n",
    "                # ----------------------------------------------------------------\n",
    "                # save results for each walking segment and for all walking segments combined \n",
    "                vid_in_path_no_ext = os.path.splitext(os.path.basename(video_id_date_name))[0]\n",
    "                \n",
    "                # stride time  \n",
    "                # each row = 1 walk segment \n",
    "                stride_time_output = os.path.join(dir_out_prefix, '005_gait_metrics', 'stride_time')\n",
    "                stride_time_stats_path = os.path.normpath(os.path.join(stride_time_output, (vid_in_path_no_ext + '_stride_time_stats_per_walk.csv')))\n",
    "                stride_time_stats_per_walk.to_csv(stride_time_stats_path)\n",
    "\n",
    "                # save stride time summary stats for all strides in all walking segments \n",
    "                stride_time_stats_all = pd.DataFrame(data = {'stride_time_mean_sec' : [stride_times_all['seconds'].mean(skipna = True)], \n",
    "                                                             'stride_time_median_sec': [stride_times_all['seconds'].median(skipna = True)], \n",
    "                                                             'stride_time_std' : [stride_times_all['seconds'].std(skipna = True)], \n",
    "                                                             'stride_time_cv' : [(stride_times_all['seconds'].std(skipna = True) / \n",
    "                                                                                   stride_times_all['seconds'].mean(skipna = True)) * 100],\n",
    "                                                             'stride_time_max' : [stride_times_all['seconds'].max(skipna = True)],\n",
    "                                                             'stride_time_min' : [stride_times_all['seconds'].min(skipna = True)]\n",
    "                                                            })\n",
    "                stride_time_all_stats_path = os.path.normpath(os.path.join(stride_time_output, (vid_in_path_no_ext + '_stride_time_stats_all.csv')))\n",
    "                stride_time_stats_all.to_csv(stride_time_all_stats_path)\n",
    "\n",
    "                # cadence \n",
    "                cadence_output_folder = os.path.join(dir_out_prefix, '005_gait_metrics', 'cadence')\n",
    "                if not os.path.exists(cadence_output_folder):\n",
    "                    os.makedirs(cadence_output_folder)\n",
    "                \n",
    "                # each row = cadence from 1 walk  \n",
    "                cadence_per_walk_path = os.path.normpath(os.path.join(cadence_output_folder, (vid_in_path_no_ext + '_cadence_per_walk.csv')))\n",
    "                cadence_per_walk.to_csv(cadence_per_walk_path)\n",
    "\n",
    "                # mean of cadence from all walk segments \n",
    "                cadence_mean_df = pd.DataFrame(index = range(1), \n",
    "                                               data = {'mean_cadence_step_per_min' : cadence_per_walk['cadence_step_per_min'].mean(skipna = True)})\n",
    "                cadence_mean_path = os.path.normpath(os.path.join(cadence_output_folder, (vid_in_path_no_ext + '_mean_cadence.csv')))\n",
    "                cadence_mean_df.to_csv(cadence_mean_path)\n",
    "\n",
    "                # stride width \n",
    "                width_output_folder = os.path.join(dir_out_prefix, '005_gait_metrics', 'stride_width')\n",
    "                if not os.path.exists(width_output_folder):\n",
    "                    os.makedirs(width_output_folder)\n",
    "\n",
    "                # save stats = each row = 1 walk \n",
    "                stride_width_stats_path = os.path.normpath(os.path.join(width_output_folder, (vid_in_path_no_ext + '_stride_width_stats_per_walk.csv')))\n",
    "                stride_width_stats_per_walk.to_csv(stride_width_stats_path)\n",
    "\n",
    "                # stats across all walks \n",
    "                stride_width_stats_all = pd.DataFrame(data = {'stride_width_mean_cm' : [stride_width_all['heel_x_diff_0'].mean(skipna = True)],\n",
    "                                                              'stride_width_median_cm' : [stride_width_all['heel_x_diff_0'].median(skipna = True)],\n",
    "                                                              'stride_width_std' : [stride_width_all['heel_x_diff_0'].std(skipna = True)], \n",
    "                                                              'stride_width_cv' : [(stride_width_all['heel_x_diff_0'].std(skipna = True) / \n",
    "                                                                                    stride_width_all['heel_x_diff_0'].mean(skipna = True)) * 100],\n",
    "                                                              'stride_width_max' : [stride_width_all['heel_x_diff_0'].max(skipna = True)],\n",
    "                                                              'stride_width_min' : [stride_width_all['heel_x_diff_0'].min(skipna = True)]\n",
    "                                                              })\n",
    "                \n",
    "                stride_width_all_stats_path = os.path.normpath(os.path.join(width_output_folder, (vid_in_path_no_ext + '_stride_width_stats_all.csv')))\n",
    "                stride_width_stats_all.to_csv(stride_width_all_stats_path)\n",
    "\n",
    "                # double + single support  \n",
    "                #if all values = 0 --> wasn't able to calculate support for any walking segments \n",
    "                if sum(support_for_any_segments) == 0: \n",
    "                    vids_support_not_calculated.append(video_id_date_name)\n",
    "                    support_metrics_all = create_blank_df_for_no_support()\n",
    "                    \n",
    "\n",
    "                else: # metrics were calculated \n",
    "                    vids_support_calculated.append(video_id_date_name)\n",
    "                    \n",
    "                    support_output_folder = os.path.join(dir_out_prefix, '005_gait_metrics', 'support')\n",
    "                    if not os.path.exists(support_output_folder):\n",
    "                        os.makedirs(support_output_folder)\n",
    "\n",
    "                    # one row per walk \n",
    "                    support_metrics_per_walk_path = os.path.normpath(os.path.join(support_output_folder, (vid_in_path_no_ext + '_support_metrics_per_walk.csv')))\n",
    "                    support_metrics_per_walk.to_csv(support_metrics_per_walk_path)\n",
    "\n",
    "                    # mean of each row \n",
    "                    support_metrics_all = pd.DataFrame(support_metrics_per_walk.mean(numeric_only = True)).transpose()\n",
    "                    support_metrics_all.columns = [f\"{col}_mean\" for col in support_metrics_all.columns]\n",
    "                    support_metrics_all['walk_segment'] = support_metrics_per_walk['walk_segment'].iloc[0]\n",
    "                    support_metrics_all['foot1'] = support_metrics_per_walk['foot1'].iloc[0]\n",
    "                    support_metrics_all_path =  os.path.normpath(os.path.join(support_output_folder, (vid_in_path_no_ext + '_support_metrics_all_mean.csv')))\n",
    "                    support_metrics_all.to_csv(support_metrics_all_path)\n",
    "                    \n",
    "        # if valid segments found, compile all metrics stats for one participant into one .csv and save for all participants df \n",
    "        if valid_segments_found == 1:\n",
    "            pose_metrics_df = save_all_pose_metrics(id_date, video_id_date_name, task,\n",
    "                                                    stride_time_stats_all, \n",
    "                                                    cadence_mean_df, \n",
    "                                                    stride_time_stats_df, \n",
    "                                                    stride_width_stats_all,\n",
    "                                                    support_metrics_all,\n",
    "                                                    dir_out_prefix)\n",
    "\n",
    "            all_participants_pose_metrics.append(pose_metrics_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde68611-29b3-4242-9c71-d4f7f64acdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of all participants ----------------------------------------------------------------------\n",
    "all_participants_pose_metrics_df = pd.concat(all_participants_pose_metrics, ignore_index = True)\n",
    "raw_parent, all_gait_metrics_filename = os.path.split(raw_pose_data_in_path)\n",
    "all_participants_pose_metrics_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_pose_metrics_all.csv')))\n",
    "print('Saving ' +  os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_pose_metrics_all.csv')) )\n",
    "\n",
    "# save file on included vs excluded walks \n",
    "excluded_videos_df = pd.DataFrame(data = {'excluded_vids' : excluded_videos})\n",
    "excluded_videos_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_excluded_videos.csv')))\n",
    "\n",
    "included_videos_df = pd.DataFrame(data = {'included_vids' : included_videos})\n",
    "included_videos_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_included_videos.csv')))\n",
    "\n",
    "# save file with videos for which double support could and couldn't be calculated \n",
    "vids_support_calculated_df = pd.DataFrame(data = {'vids_support_calculated' : vids_support_calculated})\n",
    "vids_support_calculated_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_support_calculated.csv')))\n",
    "\n",
    "\n",
    "vids_support_not_calculated_df = pd.DataFrame(data = {'vids_support_not_calculated' : vids_support_not_calculated})\n",
    "vids_support_not_calculated_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_support_not_calculated.csv')))\n",
    "\n",
    "# save list of videos without enough .csv input files for analysis (often have fps but missing pose data) \n",
    "vids_insufficient_data_df = pd.DataFrame(data = {'vids_insufficient_data' : vids_insufficient_data})\n",
    "vids_insufficient_data_df.to_csv(os.path.join(raw_pose_data_in_path, (all_gait_metrics_filename + '_insufficient_data_for_gait_analysis.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020dcfa1-73cc-44a8-8a7f-94ae7d388673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of vids_insufficient_data_df')\n",
    "print(len(vids_insufficient_data_df))\n",
    "\n",
    "print('number of excluded_videos_df')\n",
    "print(len(excluded_videos_df))\n",
    "\n",
    "print('number of included_videos_df')\n",
    "print(len(included_videos_df))\n",
    "\n",
    "print('number of vids_support_calculated_df')\n",
    "print(len(vids_support_calculated_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_home_video_analysis_2",
   "language": "python",
   "name": "venv_home_video_analysis_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
